





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.model_selection import train_test_split
from sklearn.metrics import root_mean_squared_error
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import StandardScaler


# Show all columns
pd.set_option('display.max_columns', None)

# Show all rows
pd.set_option('display.max_rows', None)

# Optional: widen display so lines donâ€™t wrap
pd.set_option('display.width', None)
pd.set_option('display.max_colwidth', None)


plt.rcParams['figure.figsize'] = (10, 5)
# sets default size for figures





df_org = pd.read_csv('../data/cleaned_ameshousing.csv')
# imports the original cleaned dataset (before subsetting)


columns = ['Overall Qual_codes', 'Exter Qual_codes', 'Kitchen Qual_codes', 'TotRms AbvGrd', 'Total Full Bath', 'Neighborhood', 'Sale Type', 'House Style', 'SalePrice']
# these are the columns I choose to simplify the model
# these columns were chosen for their correlation coefficent to Sale Price being very strong (for the ordinal columns)
# the nominal columns were also chosen for their logic connections (neighborhood = location, sale type = type of sale, house style = house frame, like number of stories)


df = df_org[columns]
# creates the dataframe to use for EDA


# df['Overall Qual_codes'] = df['Overall Qual_codes'] + 1
# adjusting this, as the codes were off by 1. Fine for modeling, but not for visualizing 


df.head()
# first 5 rows of the dataframe


df_dummies = pd.get_dummies(df, columns=['Neighborhood', 'Sale Type', 'House Style']) 
# this is for modeling. it makes the dummy variables for the nominal (non-numerical) columns. 





df.groupby('Neighborhood')['SalePrice'].agg(['mean', 'median', 'count']).sort_values(by=['mean', 'median'], ascending=[False, False])
# groups the dataframe by neighborhood, calculating the mean, median, and count for each neighborhood


ax = sns.barplot(data=df, x='Neighborhood', y='SalePrice', errorbar=None)
plt.xticks(rotation=45)
plt.title("Average Sale Price by Neighborhood");
# This creates a barplot of the average sale price by neighborhood





ax = sns.countplot(data=df, x='Neighborhood')
ax.bar_label(ax.containers[0])
plt.xticks(rotation=45)
plt.title("# of Homes in each Neighborhood");
# this makes a countplot of the number of houses in each neighborhood





ax = sns.barplot(data=df, x='Neighborhood', y='Overall Qual_codes', errorbar=None)
for container in ax.containers:
    # Use '%.2f' to round the labels to two decimal places
    ax.bar_label(container, fmt='%.2f') 
plt.title("Average overall quality by Neighborhood")
plt.xticks(rotation=45);
# this makes a barplot of the averagw overall quality of houses in each neighborhood. 





sns.barplot(data=df, x='House Style', y='SalePrice', errorbar=None)
plt.title("Average Sale Price by House Style")
plt.xticks(rotation=45);
# makes a barplot of the average sale price by style of house. 





ax = sns.countplot(data=df, x='House Style')
plt.title("Number of different House Styles")
ax.bar_label(ax.containers[0]);
# makes a countplot of the different house styles





ax = sns.barplot(data=df, x='House Style', y='Overall Qual_codes', errorbar=None)
for container in ax.containers:
    ax.bar_label(container, fmt='%.2f') 
plt.title("Average overall quality by House Style")
plt.xticks(rotation=45);
# creates a barplotof the average overall quality by house style











lr = LinearRegression()
# initializes the linear regression model
rf = RandomForestRegressor()
# initializes the random forest model
dt = DecisionTreeRegressor()
# initializes the decision tree model


X = df_dummies.drop(columns='SalePrice')
# this is our features (with nominal columns as dummy variables)
y = df_dummies['SalePrice']
# this is our target variable


models = {'Linear Regression': lr, 'Random Forest': rf, 'Decision Tree': dt}
# made a dictionary of the models to loop through 


sizes = np.arange(0.2, 0.4, 0.01).round(2)
# different sizes to use for test size


r2_scores = []
# array to store r2 score dictionaries
rmse_scores = []
# array to store rmse score dictionaries
for size in sizes:
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=size)
    # splits the data into training data and test data
    for name, model in models.items():
        model.fit(X_train, y_train)
        # fits each model
    r2_scores.append({name: model.score(X_test, y_test) for name, model in models.items()})
    # appends the r2 score of each model to the r2_scores list
    rmse_score = {}
    # starts the dictionary to be appended to rmse_scores
    for name, model in models.items():
        y_preds = model.predict(X_test)
        # makes predictions on the X_test set
        y_mean = y_test.mean()
        # calculates the mean of y_test
        baseline_preds = np.full_like(y_test, y_mean)
        # baseline prediction (every prediction is the mean)
        rmse_score[name] = root_mean_squared_error(y_test, y_preds)
        # assigns teh RMSE score to the correct model name
    rmse_score['Baseline'] = root_mean_squared_error(y_test, baseline_preds)
    # adds the baseline score
    rmse_scores.append(rmse_score)
    # appends the rmse_score dictionary to teh rmse_scores list

r2_scores_df = pd.DataFrame(r2_scores)
rmse_scores_df = pd.DataFrame(rmse_scores)
# the above 2 lines make dataframes out of the r2 and rmse scores lists


r2_scores_df['test_size'] = sizes
rmse_scores_df['test_size'] = sizes
# adds a new column to the dataframes, sizes, which contains the different test sizes 


r2_scores_df
# output of r2_scores_df


rmse_scores_df
# output of the rmse_scores_df


sns.lineplot(data=r2_scores_df, x='test_size', y='Linear Regression', label='Linear Regression', marker='o')
sns.lineplot(data=r2_scores_df, x='test_size', y='Random Forest', label='Random Forest', marker='o')
sns.lineplot(data=r2_scores_df, x='test_size', y='Decision Tree', label='Decision Tree', marker='o')
plt.xlabel("Test Size")
plt.ylabel("R-squared Score")
plt.title("R-squared scores at different test sizes")
plt.xticks(sizes);
# creates multiple lineplots to see how each model performed wuth R2 with differnt test sizes (x-label)





sns.lineplot(data=rmse_scores_df, x='test_size', y='Linear Regression', label='Linear Regression', marker='o')
sns.lineplot(data=rmse_scores_df, x='test_size', y='Random Forest', label='Random Forest', marker='o')
sns.lineplot(data=rmse_scores_df, x='test_size', y='Decision Tree', label='Decision Tree', marker='o')
sns.lineplot(data=rmse_scores_df, x='test_size', y='Baseline', label='Baseline', marker='o')
plt.xlabel("Test Size")
plt.ylabel("RMSE Score")
plt.title("RMSE scores at different test sizes")
plt.xticks(sizes);
# creates multiple lineplots to see how each model performed with RMSE with differnt test sizes (x-label)








X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.25)
# splits the data into training and test sets. 75/25 split


sc =StandardScaler()
# initializes scaler to scale the features
X_train_sc = sc.fit_transform(X_train)
# fits the scaler to X_train and transforms it, storing the result in X_train_sc
X_test_sc = sc.transform(X_test)
# transforms X_test and stores the result in X_test_sc


scores = []
rmse_scores = []
# list to keep track of scores for different number of neighbors


for n in range(3, 30, 2):
    # loop through the odd numbers from 3 to 30 for differnt neighbor numbers
    knn = KNeighborsRegressor(n_neighbors=n)
    # initializes the KNN regression with specified number of neighbors
    knn.fit(X_train_sc, y_train)
    # fits the data to the knn model
    scores.append({'neighbors': n, 'score': knn.score(X_test_sc, y_test)})
    # appends the number of neighbors nad the socre to the scores list
    knn_preds = knn.predict(X_test_sc)
    # predictions of the knn model on the test set
    baseline_preds = np.full_like(y_test, y_test.mean())
    # bsaeline predictions (uses the mean for every perdiction)
    rmse_scores.append({'neighbors': n, 'score': root_mean_squared_error(y_test, knn_preds), 'baseline': root_mean_squared_error(y_test, baseline_preds)})
    # appends teh dictioanry of the neighbors, rmse score, and baseline score
    


scores_df = pd.DataFrame(scores)
rmse_scores_df = pd.DataFrame(rmse_scores)
# makes a  dataframe out of the scores list. 


scores_df
# shows the R2 score of different number of neighbors


rmse_scores_df
# shows the RMSE score of different number of neighbors


plt.plot(scores_df['neighbors'], scores_df['score'], marker='o')
plt.xlabel("Neighbors")
plt.ylabel("R2 Score")
plt.title("R2 Scores at different K values")
plt.xticks(range(3, 30, 2));
# plots the line showing how well the model did (R2 score) with differnt number of neighbors





plt.plot(rmse_scores_df['neighbors'], rmse_scores_df['score'], marker='o')
plt.xlabel("Neighbors")
plt.ylabel("RMSE Score")
plt.title("RMSE Scores at different K values")
plt.xticks(range(3, 30, 2));
# plots the line showing how well the model did (RMSE score) with differnt number of neighbors












